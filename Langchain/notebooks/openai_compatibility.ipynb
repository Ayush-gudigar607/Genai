{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "381cf30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (1.2.7)\n",
      "Requirement already satisfied: langchain-openai in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (1.1.7)\n",
      "Requirement already satisfied: openai in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (2.16.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.7 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langchain) (1.2.7)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.7 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langchain) (1.0.7)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langchain) (2.12.5)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (0.6.7)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.7->langchain) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langgraph<1.1.0,>=1.0.7->langchain) (4.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langgraph<1.1.0,>=1.0.7->langchain) (1.0.7)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langgraph<1.1.0,>=1.0.7->langchain) (0.3.3)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langgraph<1.1.0,>=1.0.7->langchain) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.7->langchain) (1.12.2)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (3.11.6)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: sniffio in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from openai) (4.67.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2026.1.15)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (2.6.3)\n",
      "Requirement already satisfied: colorama in c:\\Users\\USER\\Desktop\\GenAi\\.venv\\Lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install  langchain langchain-openai openai python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c5177af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"AIzaSyDLXj0QaYgPQWUJNbphdpPNwxWOHUt4w3g\"\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://generativelanguage.googleapis.com/v1beta/openai/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b349dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"gemini-2.5-flash\",temperature=0.2)\n",
    "\n",
    "llm.invoke(\"who is the president of the united states?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed94dd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**RAG stands for Retrieval-Augmented Generation.**\n",
      "\n",
      "It's a technique that enhances the capabilities of Large Language Models (LLMs) by giving them access to external, up-to-date, and domain-specific information *before* they generate a response.\n",
      "\n",
      "Think of it as giving an LLM a \"cheat sheet\" or a \"research assistant\" before it answers a question.\n",
      "\n",
      "---\n",
      "\n",
      "### The Problem RAG Solves (Why RAG?)\n",
      "\n",
      "Traditional LLMs, while powerful, have several limitations:\n",
      "\n",
      "1.  **Knowledge Cutoff:** Their knowledge is limited to the data they were trained on, which means they can't access real-time or very recent information.\n",
      "2.  **Hallucinations:** LLMs can sometimes \"make things up\" or generate factually incorrect information, especially when asked about topics outside their training data or when they lack confidence.\n",
      "3.  **Lack of Domain-Specific Knowledge:** A general-purpose LLM won't know the specifics of your company's internal documents, proprietary data, or niche industry jargon.\n",
      "4.  **Lack of Transparency/Explainability:** It's hard to trace where an LLM's answer came from, making it difficult to verify its accuracy.\n",
      "5.  **Cost of Retraining:** Continuously retraining an LLM with new data is extremely expensive and time-consuming.\n",
      "\n",
      "RAG addresses these issues by providing LLMs with a mechanism to look up relevant information from a reliable source at the time of query.\n",
      "\n",
      "---\n",
      "\n",
      "### How RAG Works (Step-by-Step)\n",
      "\n",
      "RAG typically involves two main phases: **Indexing (offline preparation)** and **Retrieval & Generation (online at runtime)**.\n",
      "\n",
      "#### Phase 1: Indexing/Preparation (Offline)\n",
      "\n",
      "This phase prepares your external knowledge base for efficient retrieval.\n",
      "\n",
      "1.  **Data Ingestion:** Collect all the relevant documents, articles, databases, web pages, etc., that you want your LLM to be able to reference. This is your \"knowledge corpus.\"\n",
      "2.  **Chunking:** Break down these large documents into smaller, manageable \"chunks\" or segments. This is crucial because an LLM's context window (the amount of text it can process at once) is limited, and smaller chunks are easier to search.\n",
      "3.  **Embedding:** Each chunk of text is then converted into a numerical representation called a \"vector embedding\" using an embedding model. These embeddings capture the semantic meaning of the text, meaning similar chunks of text will have similar vector representations.\n",
      "4.  **Vector Database Storage:** These vector embeddings (along with a reference back to their original text chunks) are stored in a specialized database called a **Vector Database** (or Vector Store). This database is optimized for fast similarity searches.\n",
      "\n",
      "#### Phase 2: Retrieval & Generation (Online/Runtime)\n",
      "\n",
      "This phase happens when a user asks a question.\n",
      "\n",
      "1.  **User Query:** A user submits a question or prompt (e.g., \"What are the Q3 earnings for Acme Corp?\").\n",
      "2.  **Query Embedding:** The user's query is also converted into a vector embedding using the *same* embedding model used during indexing.\n",
      "3.  **Similarity Search (Retrieval):** The query's embedding is used to perform a similarity search in the Vector Database. The system finds the top 'k' most semantically similar chunks of text from your knowledge base. These are the \"retrieved documents\" or \"context.\"\n",
      "4.  **Context Augmentation:** The retrieved chunks of text are then added to the original user query, forming an \"augmented prompt.\"\n",
      "    *   *Example Augmented Prompt:* \"Based on the following information: [Retrieved Chunk 1], [Retrieved Chunk 2], [Retrieved Chunk 3], please answer: What are the Q3 earnings for Acme Corp?\"\n",
      "5.  **LLM Generation:** This augmented prompt is sent to the LLM. The LLM now has the specific, relevant information it needs to formulate an accurate and grounded answer.\n",
      "6.  **Response:** The LLM generates a response based on the provided context and its own general knowledge. Often, the system can also cite the sources (the original documents from which the chunks were retrieved).\n",
      "\n",
      "---\n",
      "\n",
      "### Key Components of a RAG System\n",
      "\n",
      "*   **Knowledge Base/Corpus:** The collection of external data.\n",
      "*   **Chunking Strategy:** How documents are split (e.g., fixed size, semantic splitting).\n",
      "*   **Embedding Model:** Converts text to numerical vectors (e.g., OpenAI's `text-embedding-ada-002`, various open-source models).\n",
      "*   **Vector Database:** Stores and indexes vector embeddings for fast similarity search (e.g., Pinecone, Weaviate, Chroma, FAISS).\n",
      "*   **Retrieval Mechanism:** The algorithm used to find similar vectors (e.g., cosine similarity, dot product).\n",
      "*   **Large Language Model (LLM):** The generative model (e.g., GPT-3.5, GPT-4, Llama 2, Claude).\n",
      "*   **Prompt Engineering:** Crafting the prompt to effectively integrate the retrieved context.\n",
      "\n",
      "---\n",
      "\n",
      "### Benefits of RAG\n",
      "\n",
      "*   **Reduced Hallucinations:** Answers are grounded in factual, verifiable information.\n",
      "*   **Access to Up-to-Date Information:** Can pull from live databases or recently updated documents.\n",
      "*   **Domain-Specific Knowledge:** Tailors LLMs to specific organizational or industry needs without retraining.\n",
      "*   **Improved Accuracy & Reliability:** More trustworthy and precise answers.\n",
      "*   **Transparency & Explainability:** Can often provide sources for the information used in the answer.\n",
      "*   **Cost-Effective:** Avoids the need for expensive and frequent LLM retraining.\n",
      "*   **Faster Development:** Easier to adapt LLMs to new data sources.\n",
      "\n",
      "---\n",
      "\n",
      "### Limitations and Challenges of RAG\n",
      "\n",
      "*   **Quality of Retrieved Data:** \"Garbage in, garbage out.\" If the knowledge base is poor or irrelevant, the answers will suffer.\n",
      "*   **Chunking Strategy:** Poor chunking can lead to missing context or irrelevant information being retrieved.\n",
      "*   **Embedding Model Choice:** The quality of embeddings directly impacts retrieval accuracy.\n",
      "*   **Vector Database Performance:** Scalability and latency can be issues with very large knowledge bases.\n",
      "*   **Prompt Engineering Complexity:** Crafting effective prompts that integrate retrieved context seamlessly can be challenging.\n",
      "*   **Latency:** The retrieval step adds a small amount of latency to the overall response time.\n",
      "*   **Security & Privacy:** Handling sensitive data in the knowledge base and ensuring secure retrieval.\n",
      "\n",
      "---\n",
      "\n",
      "### Common Use Cases for RAG\n",
      "\n",
      "*   **Customer Support Chatbots:** Answering customer questions based on product manuals, FAQs, and support tickets.\n",
      "*   **Enterprise Search:** Allowing employees to query internal documents, policies, and knowledge bases.\n",
      "*   **Research & Development:** Helping researchers find relevant papers, patents, and experimental data.\n",
      "*   **Personalized Content Generation:** Creating tailored content based on user preferences and historical data.\n",
      "*   **Legal & Medical Information Systems:** Providing accurate and up-to-date information from legal codes or medical journals.\n",
      "*   **Educational Tools:** Answering student questions based on course materials and textbooks.\n",
      "\n",
      "In essence, RAG is a powerful paradigm that makes LLMs far more practical, reliable, and trustworthy for real-world applications by giving them the ability to \"look things up\" before speaking.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "res=llm.invoke([HumanMessage(content=\"Explain the RAG\")])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e24f13",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
